---
title: "PML Project"
author: "mapologo"
date: "22 de febrero de 2015"
output: html_document
---

# Motivation

We use data form Human Activity Recognition project (HAR) from Groupware
Research Group to recognize how well barbell liftws are made using data from
accelerometers on the belt, forearm, arm, and dumbell of 6 participants.

http://groupware.les.inf.puc-rio.br/har

# Load and process Data

```{r, cache=TRUE}
setwd("~/projects/predmachlearn-011/")
pml_training <- read.csv("data/pml-training.csv")
# summary(pml_training)
```

Summary() funtion shows that there are empty values in fields as "min_yaw_belt"
and "#DIV/0!" values in fields as "min_yaw_forearm" that make that these fields
are taken as factor. So, this values has to be read as NA.

```{r, cache=TRUE}
setwd("~/projects/predmachlearn-011/")
pml_training <- read.csv("data/pml-training.csv", na.strings=c("NA","#DIV/0!",""))
# summary(pml_training) # no more incorrect factor fields
```
Some columns had a lot of missing values and the others none. So take only
columns without missing values.

```{r, fig.height=3, fig.width=6}
library(ggplot2)
qplot(1:ncol(pml_training), colSums(is.na(pml_training)), xlab = "col", ylab = "# of NAs")
```

```{r}
predictors <- colnames(pml_training)
predictors <- predictors[colSums(is.na(pml_training)) == 0] # columns without missing values
```

First seven columns are description, time and window data, therefore are not relevant

```{r}
predictors <- predictors[-(1:7)]
```

With this selection we have gone from 160 to 53 columns. All the remaining predictors are not near zero variance.

```{r}
library("caret")
# nearZeroVar(pml_training[, predictors], saveMetrics = TRUE)
```

We take our final data set:

```{r}
pml_training <- pml_training[, predictors]
```

Load the testing data to use our final model for blind prediction:

```{r, cache=TRUE}
setwd("~/projects/predmachlearn-011/")
pml_testing <- read.csv("data/pml-testing.csv", na.strings=c("NA","#DIV/0!",""))
pml_testing <- pml_testing[, predictors[1:52]]
```
library("pander")
pander(head(pml_training[,1:10]))

# Exploratory Analysis

We want to see how our predictors behave, as we have many variables (52), we
separated them in 4 subsets of 13 for "bet", "dumbbell", "arm" and "forearm"
variables:

```{r}
beltC <- c(grep("belt", predictors), 53)
dumbC <- c(grep("dumbbell", predictors), 53)
armC <- c(grep("[^(fore)]arm", predictors), 53)
foreC <- c(grep("forearm", predictors), 53)
```

Then we plot them using violen geometry:

```{r, cache=TRUE}
library(reshape2)

groupViolin <- function(groupC){
    groupM <- melt(pml_training[, groupC], id.vars = "classe")
    ggplot(groupM, aes(x=classe, y=value)) +
        geom_violin(aes(color=classe, fill=classe), alpha=1/2) +
        facet_wrap(~ variable, scale="free_y") +
        scale_color_brewer(palette="Set1") +
        scale_fill_brewer(palette="Set1") +
        labs(x="", y="") +
        theme(legend.position="none")
}

groupViolin(beltC)
groupViolin(dumbC)
groupViolin(armC)
groupViolin(foreC)
```

# Train parameters and data partition

We set our train parameters as 5-fold repeated cross validation across
different models:

```{r}
library(caret)

# Train control across models
trCtrl <- trainControl(method="repeatedcv", number=5, repeats=1, 
                       verboseIter=FALSE)
```

And split pml_training dataset in 60% training and 40% testing.

```{r}
## set seed
set.seed(300369)

inTrain <- createDataPartition(y=pml_training$classe, p=0.6, list=FALSE)
training <- pml_training[inTrain, ]
testing <- pml_training[-inTrain, ]
dim(training)
dim(testing)
```
# Preprocessing

To avoid numerical annoyances, calculate centering and scaling over training
data, and apply to training and testing subsets.

```{r}
library("data.table")

preProc <- preProcess(training[1:52])
predppTrain <- predict(preProc, training[1:52])
trainingCS <- data.table(data.frame(classe = training$classe, predppTrain))

predppTest <- predict(preProc, testing[1:52])
testingCS <- data.table(data.frame(classe = testing$classe, predppTest))
```

# Decision tree

We start trying with a simple decision tree model.

```{r, cache=TRUE}
system.time(treeFit <- train(classe ~., data=trainingCS, method="rpart", trControl=trCtrl))
treePred <- predict(treeFit, newdata=testingCS)
treeCM <- confusionMatrix(treePred, testingCS$classe)
treeCM
```

For pedagogical reasons we present a plot given by `rattle` package.

```{r}
library("rattle")
library("rpart.plot")
fancyRpartPlot(treeFit$finalModel)
```

This fancy model only gives `r sprintf(treeCM$overall["Accuracy"]*100, fmt = "%.2f")` % of Accuracy.

# Linear Discriminant Analysis

We try also with a fast Linear Discriminant Analysis model.

```{r, cache=TRUE}
system.time(ldaFit <- train(classe ~., data=trainingCS, method="lda", trControl=trCtrl))
ldaPred <- predict(ldaFit, newdata=testingCS)
ldaCM <- confusionMatrix(ldaPred, testingCS$classe)
ldaCM
```

This faster model only gives `r sprintf(ldaCM$overall["Accuracy"]*100, fmt = "%.2f")` % of Accuracy.

# Random Forest

Finally, a more powerful random forest model: 

```{r, cache=TRUE}
system.time(rfFit <- train(classe ~., data=trainingCS, method="rf", trControl=trCtrl))
rfPred <- predict(rfFit, newdata=testingCS)
rfCM <- confusionMatrix(rfPred, testingCS$classe)
rfCM
```

This slower model gives an impressive `r sprintf(rfCM$overall["Accuracy"]*100, fmt = "%.2f")` % of Accuracy.

# Apply models to the 20 test cases

We apply our models to *pml_testing* data:

```{r}
pml_testingCS <- predict(preProc, pml_testing)
treePredTesting <- predict(treeFit, newdata=pml_testingCS)
treePredTesting
ldaPredTesting <- predict(ldaFit, newdata=pml_testingCS)
ldaPredTesting
rfPredTesting <- predict(rfFit, newdata=pml_testingCS)
rfPredTesting
```

# Conclusions

Random Forest gives a high accuraccy of 99% to fit accelerometer data. Linear
Discriminant near 70% but fits the data very faster (2.5 seg in comparison of
735.5 seg of Random Forest). For a real time application with varying conditions
LDA would be a better choice.
